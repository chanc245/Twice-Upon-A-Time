// index_may6.js
//ver20 may6
//conbining index_arduino + index_apr8_mvp

import dotenv from "dotenv";
import express from "express";
import cors from "cors";
import { fileURLToPath } from "url";
import { dirname, join } from "path";
import http from "http";
import { WebSocketServer, WebSocket } from "ws";
import * as mic from "./shared/mic.js";
import * as audio from "./shared/audio.js";
import * as elevenlab from "./shared/elevenlab.js";
import * as glo from "./shared/gloVariable.js";
import * as openai from "./shared/openai.js";
import * as server from "./shared/server.js";
import { getGptResultAsString } from "./shared/openai.js";
import { convertTextToSpeech } from "./shared/elevenlab.js";
import path from "path";

dotenv.config();

// Initialize variables
const folderPath = "./audio/";
let userAudioFiles = [];
let gptAudioFiles = [];
let otherKeepFiles = ["user_filelist.txt"];
let transcriptionArchives = [];
let gptResponseArchives = [];
const userAudioFilesCombineNum = 6;
let VOICE_ID = "ZF6FPAbjXT4488VcRRnw";
let voiceIDList = [];
let voiceIDDeleteList = [];
let userCloneNum = 0;
let currentStatus = "";

// Express setup
const app = express();
const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

app.use(cors());
app.use(express.json());
app.use(express.static(join(__dirname, "./public_may6")));
app.use("/assets", express.static(join(__dirname, "./public_may6/assets")));
app.use("/audio", express.static(join(__dirname, "./audio")));

// Create HTTP server and WebSocket server
const httpServer = http.createServer(app);
const wss = new WebSocketServer({ server: httpServer });

// Store connected clients
const clients = new Set();

// Voice processing routes
app.post("/start-recording", (req, res) => {
  try {
    mic.startRecordingProcess();
    console.log("Recording started successfully");
    res.sendStatus(200);
  } catch (error) {
    console.error("Error starting recording:", error);
    res.status(500).json({ error: "Failed to start recording" });
  }
});

app.post("/stop-recording", async (req, res) => {
  try {
    mic.stopRecordingProcess();
    console.log("Recording stopped, processing...");
    
    // Get audio chunks and process them
    const audioChunks = mic.getAudioChunks();
    if (!audioChunks || audioChunks.length === 0) {
      throw new Error("No audio data recorded");
    }

    // Save audio and get transcription
    const audioFile = await audio.saveAudio(audioChunks);
    if (!audioFile) {
      throw new Error("Failed to save audio file");
    }

    console.log("ðŸ“ New audio file saved:", audioFile.filename);
    userAudioFiles.push(audioFile.filename);

    const transcription = await openai.transcribeAudio(audioFile.path);
    if (!transcription) {
      throw new Error("Failed to transcribe audio");
    }

    console.log("Transcription received:", transcription);
    transcriptionArchives.push(transcription);

    const prompt = req.body?.prompt || "You are a helpful guide.";
    console.log("ðŸ“¥ Prompt from frontend:", prompt);
    console.log("ðŸ—£ï¸ Latest transcription:", transcription);

    const fullPrompt = `${prompt}\nUser said: ${transcription}`;
    console.log("ðŸ” fullPrompt:", fullPrompt);
    const responseText = await getGptResultAsString(fullPrompt);

    res.json({ responseText });
  } catch (error) {
    console.error("âŒ Error in /stop-recording:", error);
    res.status(500).json({ error: "Failed to process voice interaction." });
  }
});

app.post("/submit", async (req, res) => {
  let input = req.body.input;

  try {
    const aiResponse = await getGptResultAsString(input);
    res.json({ ai: aiResponse });
  } catch (error) {
    console.error("Error:", error);
    res.status(500).json({
      error: "Failed to generate output. Please try again.",
    });
  }
});

app.get("/voice-id", (req, res) => {
  const voiceId = glo.getVoiceId();
  if (voiceId) {
    res.json({ voiceId });
  } else {
    res.status(404).json({ error: "No voice ID available." });
  }
});

app.post("/text-to-speech", async (req, res) => {
  try {
    const { text, voiceId = glo.getVoiceId() } = req.body;
    
    if (!text) {
      throw new Error("No text provided for text-to-speech conversion");
    }
    
    if (!voiceId) {
      throw new Error("No voice ID available for text-to-speech conversion");
    }

    console.log("Converting text to speech with voice ID:", voiceId);
    const audioFileName = await convertTextToSpeech(text, voiceId);
    const audioFilePath = path.join(folderPath, audioFileName);

    res.json({ audioFilePath });
  } catch (error) {
    console.error("Error in /text-to-speech endpoint:", error);
    res.status(500).json({ 
      error: "Text-to-speech conversion failed",
      details: error.message 
    });
  }
});

app.get("/status", async (req, res) => {
  try {
    const status = currentStatus;
    res.json({ status: status || "idle" });
  } catch (error) {
    console.error("Error fetching status:", error);
    res.status(500).json({ error: "Failed to fetch status" });
  }
});

app.get("/latest-transcription", (req, res) => {
  if (transcriptionArchives.length > 0) {
    const latestTranscription = transcriptionArchives[transcriptionArchives.length - 1];
    if (typeof latestTranscription === "string") {
      res.status(200).json({ transcription: latestTranscription });
    } else {
      console.error("Invalid transcription format:", latestTranscription);
      res.status(500).json({ error: "Invalid transcription format" });
    }
  } else {
    res.status(404).json({ error: "No transcription data available." });
  }
});

app.get("/get-stored-inputs", (req, res) => {
  try {
    const storedTexts = glo.getStoredTextInputs();
    res.json({ storedTexts });
  } catch (error) {
    console.error("Error getting stored inputs:", error);
    res.status(500).json({ error: "Failed to get stored inputs" });
  }
});

// Arduino communication endpoints
app.get("/displaySwitch", (req, res) => {
  sendMsgToArduino("displaySwitch");
  res.send("OK");
});

app.get("/act1Switch", (req, res) => {
  sendMsgToArduino("act1Switch");
  res.send("OK");
});

app.get("/act2Switch", (req, res) => {
  sendMsgToArduino("act2Switch");
  res.send("OK");
});

app.get("/act3Switch", (req, res) => {
  sendMsgToArduino("act3Switch");
  res.send("OK");
});

app.get("/act4Switch", (req, res) => {
  sendMsgToArduino("act4Switch");
  res.send("OK");
});

app.get("/char1Swch", (req, res) => {
  sendMsgToArduino("char1Swch");
  res.send("OK");
});

app.get("/char2Swch", (req, res) => {
  sendMsgToArduino("char2Swch");
  res.send("OK");
});

app.get("/char3Swch", (req, res) => {
  sendMsgToArduino("char3Swch");
  res.send("OK");
});

app.get("/char4Swch", (req, res) => {
  sendMsgToArduino("char4Swch");
  res.send("OK");
});

app.get("/char5Swch", (req, res) => {
  sendMsgToArduino("char5Swch");
  res.send("OK");
});

app.get("/allFalse", (req, res) => {
  sendMsgToArduino("allFalse");
  res.send("OK");
});

// Function to handle Arduino commands from sequence
function handleArduinoCommand(command) {
  if (command) {
    console.log(`Sending Arduino command: ${command}`);
    sendMsgToArduino(command);
  }
}

// Function to process sequence step
async function processSequenceStep(step) {
  try {
    // Handle audio if present
    if (step.audio) {
      const audioFilePath = path.join(folderPath, step.audio);
      // Play audio logic here
    }

    // Handle text if present
    if (step.text) {
      // Display text logic here
    }

    // Handle Arduino command if present
    if (step.arduino) {
      handleArduinoCommand(step.arduino);
    }

    // Handle interaction if present
    if (step.interaction) {
      // Interaction logic here
    }

    // Handle store condition if present
    if (step.store) {
      console.log("ðŸ“ Storing user inputs...");
      
      // Store the user's text input
      if (transcriptionArchives.length > 0) {
        const latestTranscription = transcriptionArchives[transcriptionArchives.length - 1];
        glo.addStoredTextInput(latestTranscription);
      }

      // Store the user's voice input (audio filename)
      console.log("ðŸŽ¤ Checking for audio files to store...");
      
      if (userAudioFiles.length > 0) {
        const latestAudioFile = userAudioFiles[userAudioFiles.length - 1];
        console.log("ðŸŽµ Found latest audio file:", latestAudioFile);
        glo.addStoredAudioFile(latestAudioFile);
        console.log("ðŸ’¾ Stored audio files:", glo.getStoredAudioFiles());

        // Only process audio and clone voice if we have enough samples
        if (userAudioFiles.length >= userAudioFilesCombineNum) {
          console.log("ðŸŽ™ï¸ Voice cloning threshold reached, starting process...");
          await processAudioAndVoiceClone();
        } else {
          console.log(`ðŸŽ™ï¸ Need ${userAudioFilesCombineNum - userAudioFiles.length} more audio samples before voice cloning`);
        }
      } else {
        console.log("âš ï¸ No audio files available to store");
      }
    }
  } catch (error) {
    console.error("Error processing sequence step:", error);
  }
}

// New function to handle audio combination and voice cloning
async function processAudioAndVoiceClone() {
  try {
    console.log("ðŸŽµ Starting audio processing and voice cloning...");
    glo.setCurrentStatus("Processing audio files...");
    currentStatus = "Processing audio files...";

    // Step 1: Combine audio files
    console.log("ðŸ”„ Combining audio files...");
    const combinedFilePath = await audio.combineAudioFiles(folderPath, userAudioFiles);
    console.log("âœ… Audio files combined successfully:", combinedFilePath);

    // Step 2: Clone voice
    console.log("ðŸŽ™ï¸ Starting voice cloning process...");
    glo.setCurrentStatus("Starting voice cloning...");
    currentStatus = "Starting voice cloning...";

    const newVoiceID = await elevenlab.cloneUserVoice(combinedFilePath, userCloneNum);
    console.log("âœ… Voice cloned successfully. New Voice ID:", newVoiceID);

    // Handle voice ID management
    const previousVoiceId = glo.getVoiceId();
    if (voiceIDList.includes(previousVoiceId)) {
      await elevenlab.deleteOldVoice(previousVoiceId, voiceIDDeleteList);
    }

    // Update voice tracking
    glo.setVoiceId(newVoiceID);
    userCloneNum += 1;
    voiceIDList.push(newVoiceID);

    console.log("ðŸŽ‰ Voice processing completed successfully!");
    glo.setCurrentStatus("Voice processing completed");
    currentStatus = "Voice processing completed";

  } catch (error) {
    console.error("âŒ Error in audio processing and voice cloning:", error);
    glo.setCurrentStatus("Error in voice processing");
    currentStatus = "Error in voice processing";
    throw error;
  }
}

// WebSocket connection handling
wss.on("connection", (ws) => {
  console.log("New client connected!");
  clients.add(ws);
  
  // Send ready message to the client
  ws.send("ready");

  // Assuming you're receiving the data in a WebSocket connection
  ws.on('message', function(data) {
    // Convert the buffer to a string
    const message = data.toString('utf8');
    console.log('Raw message received:', message);
    
    try {
      const jsonData = JSON.parse(message);
      // console.log('Parsed JSON data:', jsonData);
      
      // Handle sequence steps
      if (jsonData.type === "sequence_step") {
        // console.log('Processing sequence step:', jsonData.step);
        processSequenceStep(jsonData.step);
      }
      
      // Handle Arduino commands
      if (jsonData.type === "arduino_command") {
        console.log('Sending Arduino command:', jsonData.command);
        sendMsgToArduino(jsonData.command);
        return;
      }
      
      // Handle display switch
      if (jsonData.displaySwitch === true) {
        console.log('Display switch is ON, sending to client...');
        const clientMessage = JSON.stringify({ type: "displaySwitch", status: true });
        console.log('Sending to client:', clientMessage);
        ws.send(clientMessage);
      }
    } catch (error) {
      console.error('Error parsing JSON:', error);
    }
  });

  ws.on("close", () => {
    console.log("Client disconnected");
    console.log("---------------------");
    clients.delete(ws);
  });

  ws.on("error", (error) => {
    console.error("WebSocket error:", error);
  });
});

// Global function to send messages to all connected Arduino clients
function sendMsgToArduino(message) {
  clients.forEach((client) => {
    if (client.readyState === WebSocket.OPEN) {
      client.send(message);
      console.log(`Sent to Arduino: ${message}`);
    }
  });
}

// Voice processing functions
export const handleRecording = async () => {
  try {
    console.log("Processing recording...");
    glo.setCurrentStatus("Processing recording...");
    currentStatus = "Processing recording...";

    const audioChunks = mic.getAudioChunks();
    if (!audioChunks || audioChunks.length === 0) {
      throw new Error("No audio chunks available");
    }

    const audioFile = await audio.saveAudio(audioChunks);
    if (!audioFile) {
      throw new Error("Failed to save audio file");
    }

    console.log("ðŸ“ New audio file saved:", audioFile.filename);
    userAudioFiles.push(audioFile.filename);

    const transcription = await openai.transcribeAudio(audioFile.path);
    if (!transcription) {
      throw new Error("Failed to transcribe audio");
    }

    console.log("--TRANSCRIPTION:", transcription);
    transcriptionArchives.push(transcription);

    return transcription;
  } catch (error) {
    console.error("Error in handleRecording:", error);
    throw error;
  }
};

const handleVoiceCloning = async (transcription) => {
  if (userAudioFiles.length >= userAudioFilesCombineNum) {
    console.log("Combining user audio files...");
    glo.setCurrentStatus("Combining user audio files...");
    currentStatus = "Combining user audio files...";

    audio
      .combineAudioFiles(folderPath, userAudioFiles)
      .then(async (combinedFilePath) => {
        console.log("User audio combined successfully:", combinedFilePath);

        glo.setCurrentStatus("Start voice cloning process...");
        currentStatus = "Start voice cloning process...";

        const cloneVoicePromise = elevenlab
          .cloneUserVoice(combinedFilePath, userCloneNum)
          .then(async (newVoiceID) => {
            const previousVoiceId = glo.getVoiceId();

            if (voiceIDList.includes(previousVoiceId)) {
              await elevenlab.deleteOldVoice(
                previousVoiceId,
                voiceIDDeleteList
              );
            }

            glo.setVoiceId(newVoiceID);
            userCloneNum += 1;
            voiceIDList.push(newVoiceID);
          });

        await cloneVoicePromise;

        console.log("--Cloning completed.");
        glo.setCurrentStatus("Cloning completed...");
        currentStatus = "Cloning completed...";
      })
      .catch((err) => console.error("Error combining audio files:", err));
  } else {
    glo.setCurrentStatus(
      "Interact with system one more time to start voice cloning..."
    );
    currentStatus =
      "Interact with system one more time to start voice cloning...";
    console.log(
      `Not enough audio files to combine. At least ${userAudioFilesCombineNum} required.`
    );
  }
};

const handleCleanup = () => {
  audio.cleanupAudioFolder(folderPath, [
    ...userAudioFiles,
    ...gptAudioFiles,
    ...otherKeepFiles,
  ]);
  console.log("Cleanup completed.");
};

const debugFunctions = () => {
  console.log("-");
  console.log("--userAudioFiles:", userAudioFiles);
  console.log("--gptAudioFiles:", gptAudioFiles);
  console.log("--transcriptionArchives:", transcriptionArchives);
  console.log("--gptResponseArchives:", gptResponseArchives);
  console.log("--voiceIDList:", voiceIDList);
  console.log("--voiceIDDeleteList:", voiceIDDeleteList);
  console.log("---");
};

app.post("/combine-and-clone", async (req, res) => {
  try {
    console.log("ðŸŽµ Starting forced audio combination and voice cloning...");
    
    // Step 1: Combine audio files
    console.log("ðŸ”„ Combining audio files...");
    const combinedFilePath = await audio.combineAudioFiles(folderPath, userAudioFiles);
    console.log("âœ… Audio files combined successfully:", combinedFilePath);

    // Step 2: Clone voice
    console.log("ðŸŽ™ï¸ Starting voice cloning process...");
    const newVoiceID = await elevenlab.cloneUserVoice(combinedFilePath, userCloneNum);
    console.log("âœ… Voice cloned successfully. New Voice ID:", newVoiceID);

    // Handle voice ID management
    const previousVoiceId = glo.getVoiceId();
    if (voiceIDList.includes(previousVoiceId)) {
      await elevenlab.deleteOldVoice(previousVoiceId, voiceIDDeleteList);
    }

    // Update voice tracking
    glo.setVoiceId(newVoiceID);
    userCloneNum += 1;
    voiceIDList.push(newVoiceID);

    res.json({ newVoiceId: newVoiceID });
  } catch (error) {
    console.error("âŒ Error in combine-and-clone:", error);
    res.status(500).json({ error: "Failed to combine audio and clone voice" });
  }
});

// Start server on port 8080
const PORT = 8080;
httpServer.listen(PORT, () => {
  console.log(`------------------------------`);
  console.log(`Server is running on port ${PORT}`);
  console.log(`WebSocket server is ready`);
});
